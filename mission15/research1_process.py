# -*- coding: utf-8 -*-
"""mission15_p1_research1_done.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V0jw33m7J-jnhTXzeg1xll-NtutZN4Mm

# 1. notebook 개요

1. 환경준비 & 라이브러리 설치: 버전고정
2. 데이터 업로드 : train.csv, test.csv
3. 데이터 확인 & EDA
    - 결측치/분포/상관 등 핵심
4. 전처리 + 모델 파이프라인 학습 & RMSE 평가
5. 동일 로직의 train.py 자동생성 & 다운로드
6. requirements.txt / Dockerfile 자동 생성 & 다운로드
> 이후 로컬에서 연구자1 도커 이미지 빌드 / 푸시


    > 연구자 2 단계로 진행

# 2. 환경준비 & 라이브러리 설치
"""

# 코랩환경 준비
# Colab 전용: 런타임에 필요한 패키지 설치 (두 컨테이너와 동일 버전 유지가 핵심)
# model.pkl은 파이썬/라이브러리 버전에 민감해서, 학습(Colab)과 추론(도커) 환경의 버전을 맞추면 호환성 문제가 감소함

!pip install --quiet scikit-learn==1.4.2 pandas==2.2.2 numpy==1.26.4 joblib==1.4.2

from google.colab import drive
drive.mount('/content/drive')


"""# 3. 데이터 확인 & EDA"""

# 패키지 충돌 방지를 위해 관련 패키지 제거 후, 원하는 버전으로 "강제 재설치"
!pip uninstall -y numpy pandas scikit-learn scipy joblib
!pip install --no-cache-dir -U "numpy==1.26.4" "pandas==2.2.2" "scikit-learn==1.4.2" "joblib==1.4.2"

import pandas as pd

train_path = "/content/drive/MyDrive/AI 엔지니어 과정/Part 4/미션/Mission 15/data/mission15_train.csv"
test_path = "/content/drive/MyDrive/AI 엔지니어 과정/Part 4/미션/Mission 15/data/mission15_test.csv"

import os
print(os.path.exists(train_path), train_path)
print(os.path.exists(test_path), test_path)

# EDA : 데이터 구조/타입/결측치/분포/상관 점검
import pandas as pd

df = pd.read_csv(train_path)
print("train shape:", df.shape)
# (행, 열) 크기 확인

print("\n[헤드 5행]")
display(df.head())

print("\n[info()]")
print(df.info())

print("\n[결측치 개수]")
print(df.isna().sum())

print("\n[기술통계 - 수치형]")
display(df.describe())



# 범주형 분포(Extracurricular Activities: Yes/No)
if "Extracurricular Activities" in df.columns:
    print("\n[Extracurricular Activities 분포]")
    print(df["Extracurricular Activities"].value_counts(dropna=False))

# coalb 환경 버전 확인
import numpy, pandas, sklearn, joblib
print("numpy", numpy.__version__)
print("pandas", pandas.__version__)
print("sklearn", sklearn.__version__)
print("joblib", joblib.__version__)

# 시각화 --> 수치형 히스토그램, 상관행렬(Performance Index와 상관 체크)
import matplotlib.pyplot as plt
import numpy as np

numeric_cols = ["Hours Studied","Previous Scores","Sleep Hours","Sample Question Papers Practiced","Performance Index"]
for c in numeric_cols:
    if c in df.columns:
        df[c].hist(bins=20)
        plt.title(f"Histogram: {c}")
        plt.xlabel(c)
        plt.ylabel("count")
        plt.show()



# 상관행렬- 수치형만
print("\n[상관행렬(수치형)]")
display(df[numeric_cols].corr(numeric_only=True))

"""- 가정 : Hours Studied, Previous Score, Sample Papers가 높을수록 타깃(Performance Index)가 올라감
- Extracurricular Activities는 범주형이라 제외(원-핫 인코딩 필요함)




> **결과**
- Previous Scores가 높을수록 Performance Index가 상승함
- 나머지 칼럼의 기준 미만



# 4. 전처리 / 모델 파이프라인 학습 / RMSE 평가
"""

# 전처리(수치 표준화 + 범주 OHE)
# + 모델(RandomForestRegressor)를 하나의 파이프라인으로 학습
# 검증 RMSE 출력

import json
import joblib
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# 1. 타깃/피처 지정
target_col = "Performance Index"
feature_cols = [c for c in df.columns if c != target_col]

# 2. x, y 분리
X = df[feature_cols].copy()
y = df[target_col].copy()

# 3. 열타입 분리
numeric_features = ["Hours Studied","Previous Scores","Sleep Hours","Sample Question Papers Practiced"]
categorical_features = ["Extracurricular Activities"]


############################

# 4. 전처리 파이프라인
numeric_transformer = Pipeline(steps=[
    ("scaler", StandardScaler())
    # 수치형 표준화
])
categorical_transformer = Pipeline(steps=[
    ("ohe", OneHotEncoder(handle_unknown="ignore"))
    # 범주형 원핫 인코딩
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ],
    remainder="drop"
)


##################
# 5. 모델 정의
model = RandomForestRegressor(
    n_estimators=200,
    random_state=42,
    n_jobs=-1
)

# 6. 전처리+모델 파이프라인 결합
reg_pipeline = Pipeline(steps=[
    ("prep", preprocessor),
    ("rf", model)
])

# 7. 학습/검증 분리
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 8. 학습
reg_pipeline.fit(X_train, y_train)

# 9. 검증 - RMSE

valid_pred = reg_pipeline.predict(X_valid)
valid_rmse = mean_squared_error(y_valid, valid_pred, squared=False)
print(f"검증 RMSE: {valid_rmse:.4f}")



from sklearn.metrics import root_mean_squared_error

valid_pred = reg_pipeline.predict(X_valid)
valid_rmse = root_mean_squared_error(y_valid, valid_pred)
print(f"검증 RMSE: {valid_rmse:.4f}")

"""### 분석 : RMSE 2.2486 (목표 변수: Performance Index)

Performance Index(10~100 범위)를 고려할 때, RMSE 2.2486은 준수한 점수

### 1. 스케일 대비 분석

- **범위**: 10~100, 즉 **90의 범위**
- **RMSE**: 2.2486은 **평균 약 2.25점의 오차**를 의미
- **해석**: 90 범위 내 2.25 오차는 **매우 낮은 수준**으로, 모델이 성취도를 정확히 예측함

### 2. 실제 활용 관점

성취도 지수는 정수로 반올림됨

- RMSE 2.2486은 예측이 대부분 실제 값에서 1~3점 이내임을 의미
- 예: 실제 80, 예측 82.25 → 오차 2.25점. 학업 성취도 평가에서 **실용적으로 충분히 허용 가능**

### 3. 추가 고려 사항

성능 평가를 위한 확인 사항

- **R-squared (R2) 값**: 높을수록 모델이 데이터 분산을 잘 설명
- **데이터 특성**: 지수가 특정 범위에 집중되면 RMSE가 낮아질 수 있으므로 분포 확인 필요

**결론:**

성취도 지수(10~100) 예측에서 **RMSE 2.2486**은 **매우 준수한 성능
"""

# RMSE 대신 baseline으로 다시 비교 (평균만)
# 베이스라인: '단순히 y의 평균'만 예측했을 때 RMSE가 얼마인지 비교

from sklearn.metrics import mean_squared_error
import numpy as np

y_mean = np.full_like(y_valid, fill_value=y_train.mean(), dtype=float)  # 평균값으로만 예측
rmse_baseline = mean_squared_error(y_valid, y_mean, squared=False)
print(f"[Baseline RMSE] {rmse_baseline:.4f}")  # 이 값보다 현재 모델 RMSE가 낮으면 의미 있는 성능

"""**[RMSE VS Means 결과비교]**

###

1. **성능이 크게 개선됨.**
    - RMSE가 19.4에서 2.25로 감소했으며, 이는 약 **8.6배의 정확도가 향상된 것**
    - "평균값만 예측하는 모델"보다 실제 패턴을 훨씬 더 잘 학습
2. **데이터 특성상 매우 양호한 결과**
    - 목표 변수(Performance Index)가 **10~100 사이의 점수**라면, 평균 오차는 **약 ±2.25점**
    - 예) 실제 점수가 85점인 학생을 83~87점 사이로 예측함
3. **실험 신뢰도가 충분함**
    - 랜덤포레스트는 노이즈에 강한 모델
    - 검증 RMSE가 낮으므로 오버피팅 위험은 작음(cross-validation으로 추가 확인을 권장).
"""

# 베이스 라인 vs 교차검증

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, mean_squared_error
import numpy as np

# RMSE 음수 스코어로 변환(사이킷런은 '클수록 좋음' 규칙이라, -RMSE를 사용)
rmse_scorer = make_scorer(mean_squared_error, squared=False, greater_is_better=False)
cv_scores = cross_val_score(reg_pipeline, X, y, cv=5, scoring=rmse_scorer, n_jobs=-1)
print("CV RMSE:", -cv_scores)                 # 각 폴드의 RMSE
print("CV RMSE mean:", -cv_scores.mean())     # 평균 RMSE

# 피처 중요도 확인

# 전처리 후 피처 이름 얻기 (OHE까지 적용된 최종 컬럼명)
ohe = reg_pipeline.named_steps['prep'].named_transformers_['cat'].named_steps['ohe']
num_cols = ["Hours Studied","Previous Scores","Sleep Hours","Sample Question Papers Practiced"]
cat_cols = list(ohe.get_feature_names_out(["Extracurricular Activities"]))
final_feature_names = num_cols + cat_cols

# 랜덤포레스트의 feature_importances_
importances = reg_pipeline.named_steps['rf'].feature_importances_

# 중요도 테이블
import pandas as pd
fi = pd.DataFrame({"feature": final_feature_names, "importance": importances}).sort_values("importance", ascending=False)
fi.head(10)

# 오차 분석 --> 예측이 크게 빗나간 케이스를 찾아서 데이터 품질 & 피처 설계 개선점 포착

valid_resid = y_valid - valid_pred
abs_err = (valid_resid.abs()).reset_index(drop=True)
top_err_idx = abs_err.sort_values(ascending=False).head(5).index
pd.DataFrame({"y_true": y_valid.reset_index(drop=True).iloc[top_err_idx],
              "y_pred": pd.Series(valid_pred).iloc[top_err_idx],
              "abs_err": abs_err.iloc[top_err_idx]})

# 하이퍼파라미터 가벼운 튜닝
# n_estimators, max_depth, min_samples_leaf 등으로 성능/속도 균형 조정.

from sklearn.model_selection import GridSearchCV

param_grid = {
    "rf__n_estimators": [200, 400],
    "rf__max_depth": [None, 10, 20],
    "rf__min_samples_leaf": [1, 2, 4]
}
gs = GridSearchCV(reg_pipeline, param_grid, cv=3, scoring=rmse_scorer, n_jobs=-1)
gs.fit(X, y)
print("Best params:", gs.best_params_)
print("Best CV RMSE:", -gs.best_score_)

"""**[성능 지표 비교 (RMSE: Root Mean Squared Error)]**



- **베이스라인 모델 (Baseline Model)**: 19.4
- **튜닝 전 모델 (Pre-Tuning Model)**: 2.25
- **튜닝 후 모델 (Post-Tuning Model) (CV RMSE)**: 2.2456

----
- 차이가 거의 없음
"""

best_model = gs.best_estimator_   # 그리드서치에서 최고 성능 모델 추출
best_model.fit(X, y)              # 전체 데이터로 다시 학습

"""# 5. model.pkl 저장"""

import joblib
joblib.dump(best_model, "model.pkl")
print("최종 모델 저장 완료")

"""# 6. train_report.json에 기록"""

import json
report = {
    "best_params": gs.best_params_,
    "cv_rmse": float(-gs.best_score_),
    "model_type": "RandomForestRegressor",
    "n_estimators": 400,
    "max_depth": None,
    "min_samples_leaf": 4
}
with open("train_report.json", "w", encoding="utf-8") as f:
    json.dump(report, f, indent=2, ensure_ascii=False)
print("✅ 학습 리포트 저장 완료")

"""# 7. model.pkl / train_report.json 다운로드"""

from google.colab import files
files.download("model.pkl")
files.download("train_report.json")

"""# 8. 동일 로직의 train.py 자동 생성 & 다운로드

학습을 노트북이 아니라 스크립트 한 방으로 수행해야 도커 ENTRYPOINT에 걸 수 있다.
"""

# 목적: 지금 노트북에서 한 학습 과정을 "train.py" 스크립트로 자동 생성
# 경로 규칙:
#   입력:  /data/train.csv, /data/test.csv
#   출력:  /artifacts/model.pkl, /artifacts/train_report.json, /artifacts/train_log.txt

train_py = r'''# -*- coding: utf-8 -*-
import os, json, shutil, joblib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

DATA_DIR = "/data"
ARTIFACTS_DIR = "/artifacts"

TRAIN_PATH = os.path.join(DATA_DIR, "train.csv")
TEST_PATH  = os.path.join(DATA_DIR, "test.csv")
MODEL_PATH = os.path.join(ARTIFACTS_DIR, "model.pkl")
REPORT_PATH = os.path.join(ARTIFACTS_DIR, "train_report.json")
LOG_PATH    = os.path.join(ARTIFACTS_DIR, "train_log.txt")

os.makedirs(ARTIFACTS_DIR, exist_ok=True)

df = pd.read_csv(TRAIN_PATH)

target_col = "Performance Index"
feature_cols = [c for c in df.columns if c != target_col]

X = df[feature_cols].copy()
y = df[target_col].copy()

numeric_features = ["Hours Studied","Previous Scores","Sleep Hours","Sample Question Papers Practiced"]
categorical_features = ["Extracurricular Activities"]

numeric_transformer = Pipeline(steps=[("scaler", StandardScaler())])
categorical_transformer = Pipeline(steps=[("ohe", OneHotEncoder(handle_unknown="ignore"))])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ],
    remainder="drop"
)

model = RandomForestRegressor(
    n_estimators=200,
    random_state=42,
    n_jobs=-1
)

reg_pipeline = Pipeline(steps=[
    ("prep", preprocessor),
    ("rf", model)
])

X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)

reg_pipeline.fit(X_train, y_train)

valid_pred = reg_pipeline.predict(X_valid)
rmse = mean_squared_error(y_valid, valid_pred, squared=False)

joblib.dump(reg_pipeline, MODEL_PATH)

report = {
    "rmse_valid": float(rmse),
    "n_train": int(len(X_train)),
    "n_valid": int(len(X_valid)),
    "model_type": "RandomForestRegressor",
    "sklearn_version": "1.4.2",
    "notes": "Preprocessor(ColumnTransformer)+RandomForest pipeline"
}
with open(REPORT_PATH, "w", encoding="utf-8") as f:
    json.dump(report, f, ensure_ascii=False, indent=2)

with open(LOG_PATH, "w", encoding="utf-8") as f:
    f.write(f"[INFO] Validation RMSE: {rmse:.4f}\n")

if os.path.exists(TEST_PATH):
    shutil.copy2(TEST_PATH, os.path.join(ARTIFACTS_DIR, "test.csv"))

print("[DONE] model.pkl 생성 및 보고서 저장 완료")
print(f"RMSE(valid) = {rmse:.4f}")
print(f"Saved: {MODEL_PATH}")
'''

with open("train.py","w",encoding="utf-8") as f:
    f.write(train_py)

from google.colab import files
files.download("train.py")
print("train.py 생성 및 다운로드 완료")

"""# 8. requirements.txt / Dockerfile 자동 생성 & 다운로드"""

# 연구자 1용 requirements.txt, Dockerfile을 자동 생성해 다운로드
# 이 파일들로 로컬에서 도커 이미지를 빌드→허브 푸시 실행(이후 단계)

req = """scikit-learn==1.4.2
pandas==2.2.2
numpy==1.26.4
joblib==1.4.2
"""

dockerfile = r'''FROM python:3.10-slim
RUN pip install --no-cache-dir --upgrade pip
WORKDIR /app
COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r requirements.txt
COPY train.py /app/train.py
VOLUME ["/data", "/artifacts"]
ENTRYPOINT ["python", "train.py"]
'''

with open("requirements.txt","w") as f:
    f.write(req)

with open("Dockerfile","w") as f:
    f.write(dockerfile)

from google.colab import files
files.download("requirements.txt")
files.download("Dockerfile")
print("requirements.txt / Dockerfile 생성 및 다운로드 완료")







